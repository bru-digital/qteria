# Implementation Prompt for Stack-Driven Framework Issues

## Context

You are implementing improvements to the **Stack-Driven Framework** - an AI-assisted product development system that takes founders from idea to working product through a systematic cascade of strategic decisions.

### Framework Overview

**Core Philosophy (from PHILOSOPHY.md):**
- **Axiom:** User Experience is the Core of Every Product
- The cascade is **generative, not prescriptive** - later sessions derive from earlier ones
- Every decision must trace back to user journey
- Technology serves journey, not the other way around

**Current Cascade (Sessions 1-6):**
1. `/refine-journey` ‚Üí Defines user journey through progressive interrogation
2. `/choose-tech-stack` ‚Üí Selects technology based on journey requirements
3. `/generate-strategy` ‚Üí Creates mission, metrics, monetization, architecture
4. `/create-design` ‚Üí Builds design system
5. `/generate-backlog` ‚Üí Creates prioritized feature backlog
6. `/create-gh-issues` ‚Üí Pushes issues to GitHub

**The Gap:**
The framework is brilliant at **strategy and planning (9/10)** but missing the **implementation bridge (5/10)**. After Session 6, users have a backlog but no:
- Code scaffold
- Database schema
- API contracts
- Testing strategy
- Go-to-market plan
- Financial model

### Professional Review Summary

A comprehensive 3-perspective review (10x Senior Full Stack Developer, Startup Entrepreneur, VC Fund Manager) scored the framework **8.5/10 as a strategic tool** but identified it as **70% of a complete solution**.

**Key findings:**
- ‚úì "Exceptionally well-architected framework"
- ‚úì "Philosophy-first architecture is a masterpiece"
- ‚úì "AI-optimized prompting is world-class"
- ‚úó "Stops at backlog - no code scaffolding"
- ‚úó "Missing go-to-market strategy"
- ‚úó "No financial modeling or unit economics"

**Review conclusion:**
> "Would I use it? Absolutely. But I'd build the missing 30%."

---

## Your Task

You are implementing issues from `REVIEW-BACKLOG.md` to complete the framework. This backlog represents the "missing 30%" that takes Stack-Driven from **70% ‚Üí 95% complete**.

---

## Implementation Standards

### 1. Command File Structure

Every slash command (`.claude/commands/*.md`) follows this format:

```markdown
# Session X: [Command Name]

**Purpose:** [One sentence - what this generates]
**When to run:** [After which previous session]
**Time required:** [Estimate]

## What This Session Creates

[Detailed description of outputs]

## Inputs (What This Reads)

- `.stack-driven/00-user-journey.md` - [What we extract from this]
- `.stack-driven/02-tech-stack.md` - [What we extract from this]
- [etc.]

## Process

### Step 1: [Action Name]

[Detailed instructions with decision trees, examples, edge cases]

**Example:**
[Code block or example showing the concept]

**Validation:**
- [ ] Checkbox criteria for this step

### Step 2: [Action Name]

[Continue pattern...]

## Output Format

The output should be saved to `.stack-driven/[NUMBER]-[NAME].md`:

\`\`\`markdown
# [Output Title]

[Template showing structure]
\`\`\`

## Quality Checklist

Before completing this session, verify:

- [ ] All inputs from previous sessions were considered
- [ ] Decisions trace back to user journey
- [ ] Output file follows naming convention
- [ ] [Command-specific quality criteria]

## What We DIDN'T Choose (And Why)

### [Alternative Approach 1]
**Why not:** [Trade-off reasoning based on journey/architecture]
**When to reconsider:** [Conditions where this becomes right choice]

### [Alternative Approach 2]
[Continue pattern...]

## Next Steps

After completing this session:
- Run `/cascade-status` to verify progress
- [Suggested next command]

## AI Agent Guidelines

[Special instructions for AI interpreting this prompt - edge cases, error handling, etc.]
```

### 2. Template File Structure

Every template (`templates/*.md`) provides the schema for outputs:

```markdown
# [Template Name]

**Purpose:** Template for [what this documents]
**Generated by:** /[command-name]
**File location:** `.stack-driven/[NUMBER]-[name].md`

---

## [Section 1]

[Markdown template with placeholders]

**Fields:**
- **[Field Name]:** [Description of what goes here]
- **[Field Name]:** [Description]

---

## [Section 2]

[Continue pattern...]

---

## Validation

This template is complete when:
- [ ] All sections are filled
- [ ] [Template-specific criteria]

## Example

[Optional: Show filled example from compliance-saas]
```

### 3. Output File Naming

All user-generated files go to `.stack-driven/` (gitignored):

```
.stack-driven/
‚îú‚îÄ‚îÄ 00-user-journey.md          # Session 1
‚îú‚îÄ‚îÄ 02-tech-stack.md            # Session 2 (or 3 in new cascade)
‚îú‚îÄ‚îÄ 02-mission.md               # Session 3 (or 4)
‚îú‚îÄ‚îÄ 03-metrics.md
‚îú‚îÄ‚îÄ 04-monetization.md
‚îú‚îÄ‚îÄ 05-architecture.md
‚îú‚îÄ‚îÄ 06-[new-session].md         # Your new commands go here
‚îú‚îÄ‚îÄ 07-[new-session].md
‚îî‚îÄ‚îÄ ...
```

**Numbering rules:**
- Core sessions: 00-XX (in execution order)
- Post-core optional: No prefix restriction
- If cascade order changes, renumber existing files

### 4. Reading Previous Outputs

Commands must parse previous session files. Example pattern:

```markdown
## Step 1: Extract User Journey Context

Read `.stack-driven/00-user-journey.md` and identify:

1. **User Persona:** Who is the target user?
   - Extract from "Who" section
   - Example: "Compliance officers at mid-size financial institutions"

2. **Critical Path:** What is the user's journey?
   - Extract journey steps (Step 1, Step 2, etc.)
   - Identify the "AHA MOMENT" (typically Step 3)

3. **Pain Points:** What frustrations exist?
   - Extract from "Situation" in journey definition
   - Example: "Manual document review takes 4 hours per assessment"

4. **Success Criteria:** What defines success?
   - Extract from "Outcome" in journey definition
   - Extract value ratio (should be ‚â•10:1)

Use this context to inform [whatever your command generates].
```

### 5. Decision Trees

Include decision trees for complex choices. Example:

```markdown
## Choosing Database Type

**Decision Tree:**

1. **Does the journey involve time-series data (metrics, logs, events)?**
   - YES ‚Üí Consider: TimescaleDB, InfluxDB
   - NO ‚Üí Continue to 2

2. **Does the journey require complex relationships between entities?**
   - YES ‚Üí Consider: PostgreSQL, MySQL
   - NO ‚Üí Continue to 3

3. **Does the journey require flexible schema (rapid iteration)?**
   - YES ‚Üí Consider: MongoDB, DynamoDB
   - NO ‚Üí Default: PostgreSQL (safest choice)

4. **What is the expected scale (from tech stack analysis)?**
   - <100K rows ‚Üí Any relational DB works
   - 100K-10M rows ‚Üí PostgreSQL with read replicas
   - >10M rows ‚Üí Consider sharding or specialized DB

**Recommended Default:** PostgreSQL
**Why:** Relational, mature, great JSON support, scales to millions of rows
**When to reconsider:** Time-series data, document-heavy, or extreme scale (>50M rows)
```

### 6. Examples in Commands

Every major section should include examples:

```markdown
### Step 3: Define API Endpoints

For each backlog epic, identify required API endpoints.

**Example (from compliance-saas):**

Epic: Document Upload & Parsing
Journey Step: Step 2 (Upload compliance document)

**Required Endpoints:**
- `POST /api/documents` - Upload document
  - Request: multipart/form-data (file)
  - Response: `{ id, status, fileName, uploadedAt }`
  - Auth: Required (user token)

- `GET /api/documents/:id/status` - Check parsing status
  - Request: Document ID
  - Response: `{ status: "parsing" | "ready" | "error", progress: 0-100 }`
  - Auth: Required (user token)

**Apply this pattern to your backlog epics.**
```

### 7. Validation Checklists

Every command needs quality checklists:

```markdown
## Quality Checklist

Before completing this session, verify:

**Journey Alignment:**
- [ ] All generated elements serve at least one journey step
- [ ] Critical path (Steps 1-3) is fully supported
- [ ] No "cool tech" that doesn't serve journey

**Completeness:**
- [ ] All backlog epics have corresponding elements
- [ ] Edge cases considered (errors, loading states, empty states)
- [ ] Performance implications documented

**Technical Soundness:**
- [ ] Choices align with tech stack decisions
- [ ] Architecture principles followed (from 05-architecture.md)
- [ ] Cost implications considered

**Documentation:**
- [ ] "What We DIDN'T Choose" section complete (2+ alternatives)
- [ ] Examples provided
- [ ] Next steps clear
```

### 8. "What We DIDN'T Choose" Sections

This is MANDATORY for all commands. It enforces critical thinking.

**Template:**

```markdown
## What We DIDN'T Choose (And Why)

### [Alternative Technology/Approach]

**What it is:** [Brief description]

**Why not (for this journey):**
- [Journey-specific reasoning]
- [Technical trade-off]
- [Cost/complexity consideration]

**When to reconsider:**
- If [specific journey requirement changes]
- If [scale/complexity threshold reached]
- If [team capability changes]

**Example:** [Concrete scenario where this would be right choice]

---

### [Another Alternative]

[Repeat pattern - aim for 2-4 alternatives per command]
```

**Example:**

```markdown
### GraphQL instead of REST

**What it is:** Query language for APIs that lets clients specify exactly what data they need

**Why not (for this journey):**
- Journey has simple CRUD operations, doesn't need complex querying
- Team is more familiar with REST (from tech stack)
- Compliance domain has well-defined data shapes (no over-fetching problem)
- REST is "boring" (framework principle: boring is beautiful for MVPs)

**When to reconsider:**
- If mobile app needs to minimize bandwidth (GraphQL reduces payload)
- If journey evolves to have complex, nested data requirements
- If building a public API (GraphQL can be better DX)

**Example:** If compliance documents had 50+ optional fields and different clients needed different subsets, GraphQL would shine.
```

---

## Command Creation Process

When implementing a new command (e.g., `/scaffold-project`):

### Phase 1: Research (30 minutes)
1. Read `PHILOSOPHY.md` - Understand core axiom
2. Read `README.md` - Understand framework positioning
3. Read existing commands - Match style and depth
4. Read backlog issue - Understand specific requirements

### Phase 2: Design (1-2 hours)
1. **Define inputs:** Which previous session files does this read?
2. **Define process:** What are the steps (aim for 4-8 steps)?
3. **Define outputs:** What file(s) does this generate?
4. **Identify decisions:** Where do we need decision trees?
5. **Find examples:** What would compliance-saas generate?

### Phase 3: Write Command (2-4 hours)
1. Create `.claude/commands/[name].md`
2. Write each section following template above
3. Add decision trees for complex choices
4. Add examples for clarity
5. Write "What We DIDN'T Choose" (2-4 alternatives)
6. Write quality checklist

### Phase 4: Write Template (1 hour)
1. Create `templates/[name].md`
2. Define output structure
3. Document all fields
4. Add validation criteria

### Phase 5: Update Integration (1 hour)
1. Update `.claude/commands/cascade-status.md` - Add new session
2. Update `README.md` - Document new command
3. Update `PHILOSOPHY.md` if adding new axioms
4. Add entry to `.claude/commands/` list

### Phase 6: Create Example (2-3 hours)
1. Run your command mentally for compliance-saas
2. Create `examples/compliance-saas/[output].md`
3. Verify example validates your command

### Phase 7: Test (1 hour)
1. Read through command as if you were AI agent
2. Check: Can I execute this unambiguously?
3. Check: Are there edge cases I'd get stuck on?
4. Add AI Agent Guidelines section if needed

---

## Cascade Reordering Process

When moving commands between core/post-core or reordering sessions:

### Step 1: Identify Impacts
1. Which commands READ this file? (Update their inputs)
2. Which commands run BEFORE this? (Renumber if needed)
3. Which commands run AFTER this? (Renumber if needed)

### Step 2: Update Command Files
1. Update session numbers in command files
2. Update "When to run" timing
3. Update "Reads" section to match new order

### Step 3: Update Template Prefixes
1. Rename output file numbers (e.g., `06-` ‚Üí `04-`)
2. Update all references to these files

### Step 4: Update cascade-status
1. Modify session list to reflect new order
2. Update core vs post-core categorization

### Step 5: Update Examples
1. Rename example files to match new numbers
2. Update content to reflect new inputs

### Step 6: Update Documentation
1. README.md - Session descriptions
2. PHILOSOPHY.md - If order reflects new principle
3. Any integration guides

---

## Specific Implementation Guidance by Issue Type

### For Development Commands (scaffold, database, API, testing)

**Key principles:**
1. **Generate real artifacts:** Not just documentation - actual migration files, config files, etc.
2. **Read architecture carefully:** These commands must respect tech stack choices
3. **Be opinionated but justified:** Provide defaults but explain why
4. **Include setup instructions:** Users need to know how to use generated files

**Example - Database Schema Command:**

```markdown
## Output Format

This command generates TWO files:

### 1. `.stack-driven/08-database-schema.md` (Documentation)
[Design decisions, ERD, rationale]

### 2. `.stack-driven/08-database-schema/migrations/` (Actual Code)

Based on tech stack (Prisma example):

\`\`\`prisma
// schema.prisma
model User {
  id        String   @id @default(cuid())
  email     String   @unique
  createdAt DateTime @default(now())

  documents Document[]
}

model Document {
  id         String   @id @default(cuid())
  fileName   String
  status     DocumentStatus
  userId     String
  user       User     @relation(fields: [userId], references: [id])
  createdAt  DateTime @default(now())
}

enum DocumentStatus {
  UPLOADING
  PARSING
  READY
  ERROR
}
\`\`\`

**Setup instructions:**
1. Copy schema.prisma to your project root
2. Run `npx prisma migrate dev --name init`
3. Run `npx prisma generate`
```

### For Strategy Commands (growth, financial, product)

**Key principles:**
1. **Data-driven:** Require quantitative inputs (market size, pricing, costs)
2. **Scenario planning:** Always include sensitivity analysis
3. **Actionable outputs:** Not just theory - specific next steps
4. **Tied to journey:** Every strategy element serves a journey step

**Example - Financial Model Command:**

```markdown
## Step 3: Calculate Unit Economics

### Customer Acquisition Cost (CAC)

**From growth strategy:**
- Channel 1: [Name] ‚Üí [Cost per acquisition estimate]
- Channel 2: [Name] ‚Üí [Cost per acquisition estimate]
- Blended CAC: [Weighted average]

**From monetization:**
- Average first-year revenue per customer: [From pricing tiers]
- CAC payback period: [Months to recover acquisition cost]

**Validation:**
- [ ] CAC < 1/3 of first-year revenue (SaaS best practice)
- [ ] Payback period < 12 months (for VC-backed)
- [ ] If not passing, adjust pricing or acquisition strategy

### Lifetime Value (LTV)

**From monetization:**
- Monthly revenue per user: [Average from pricing]
- Gross margin: [Revenue - COGS, from tech stack costs]

**Assumptions needed:**
- Expected monthly churn rate: [Default: 5% for B2B SaaS, 10% for B2C]
- Average customer lifetime: 1 / churn_rate

**Calculation:**
- LTV = (Monthly revenue √ó Gross margin) / Churn rate

**Validation:**
- [ ] LTV:CAC ratio ‚â• 3:1 (minimum for venture scale)
- [ ] Target: LTV:CAC ratio of 5:1+

### Output Format

\`\`\`markdown
## Unit Economics

| Metric | Value | Assumption | Risk |
|--------|-------|------------|------|
| CAC | $500 | Blended from SEO ($200) + Paid ($800) | SEO takes 6mo to ramp |
| LTV | $2,400 | $80/mo √ó 30mo lifetime √ó 80% margin | Churn assumption untested |
| LTV:CAC | 4.8:1 | Above | Healthy if assumptions hold |
| Payback | 6 months | CAC / (MRR √ó margin) | Acceptable for B2B |
\`\`\`
```

### For Validation Checkpoints

**Key principles:**
1. **Objective criteria:** Pass/fail should be unambiguous
2. **Evidence-based:** Require artifacts (interview notes, LOIs, surveys)
3. **Gated:** Framework should pause if validation fails
4. **Iterative:** Failed validation ‚Üí revise journey, try again

**Example - Customer Validation Checkpoint:**

```markdown
## Validation Gate: Interview 10 Target Users

**After completing `/refine-journey`, you must validate with real users before proceeding.**

### Validation Protocol

**Step 1: Recruit 10 Target Users**

From `.stack-driven/00-user-journey.md`, extract:
- User persona (who to recruit)
- Context (where to find them)

**Recruitment channels:**
- LinkedIn outreach (for B2B)
- Reddit/communities (for B2C)
- Personal network (if applicable)
- Cold email (use mission statement from journey)

**Aim for:** 10 interviews, 30 minutes each

---

**Step 2: Interview Script**

\`\`\`
Opening:
"Thanks for your time. I'm building [solution] for [persona] who struggle with [pain]. I want to understand if this resonates with your experience."

Discovery Questions:
1. Walk me through the last time you [situation from journey]
2. What was frustrating about that experience?
3. How do you currently solve this? (Existing alternatives)
4. If you could wave a magic wand, what would the ideal experience be?
5. Would you pay [price from monetization] to solve this? Why/why not?

Closing:
"Can I follow up when we have a prototype?"
\`\`\`

---

**Step 3: Analyze Interviews**

Create `.stack-driven/00-user-journey-validation.md`:

\`\`\`markdown
# User Journey Validation Results

**Interviews conducted:** [10]
**Date range:** [Start - End]

## Journey Validation Score: [1-10]

### Calculation:
- Pain point resonance: [X/10 users said yes] ‚Üí [Score 0-3]
- Willingness to pay: [Y/10 said yes] ‚Üí [Score 0-3]
- Aha moment clarity: [Z/10 understood value] ‚Üí [Score 0-4]

### Passing Criteria:
- [ ] Score ‚â• 7/10
- [ ] At least 6/10 validated pain point
- [ ] At least 5/10 willing to pay target price

## Key Insights

**What validated:**
- [Finding 1]
- [Finding 2]

**What didn't validate:**
- [Finding 1]
- [Finding 2]

**Journey revisions needed:**
- [Change 1]
- [Change 2]

## Evidence

**Interview summaries:** [Link to notes doc]
**Quotes:** [3-5 compelling quotes]
**Next steps:** [Re-run journey if score <7, proceed if ‚â•7]
\`\`\`

---

**Gate Logic:**

\`\`\`markdown
## After Validation

**If score ‚â• 7/10:**
‚úÖ Proceed to Session 2 (`/choose-tech-stack`)

**If score 4-6/10:**
‚ö†Ô∏è Revise journey based on insights
‚ö†Ô∏è Re-interview 5 users with updated journey
‚ö†Ô∏è Pass if revised score ‚â• 7

**If score < 4/10:**
‚ùå Pivot required - fundamental problem/solution mismatch
‚ùå Return to ideation
‚ùå Do not proceed without major journey revision
\`\`\`
```

---

## Quality Standards

### Command Length
- **Target:** 200-400 lines per command file
- **Minimum:** 150 lines (less = not detailed enough)
- **Maximum:** 500 lines (more = needs splitting)

### Decision Tree Coverage
- **Every major choice** should have a decision tree
- **Minimum:** 1 decision tree per command
- **Ideal:** 2-4 decision trees per command

### Examples
- **Every step** should have an example
- **Format:** "Example (from compliance-saas):"
- **Realism:** Examples must be plausible for that domain

### "What We DIDN'T Choose" Depth
- **Minimum:** 2 alternatives per command
- **Ideal:** 3-4 alternatives per command
- **Requirement:** Each alternative needs "When to reconsider"

### AI Agent Guidelines
- Include if command has edge cases
- Include if multi-step process needs clarification
- Include if common mistakes expected

---

## Testing Your Command

Before considering a command complete:

### Self-Review Checklist
- [ ] Read command as if you were an AI agent - can you execute it?
- [ ] Verify: Does every decision have clear criteria?
- [ ] Verify: Are there examples for complex steps?
- [ ] Verify: Is "What We DIDN'T Choose" complete?
- [ ] Verify: Does this maintain cascade (reads previous, writes for next)?

### Cascade Integration Test
- [ ] Update `/cascade-status` - does new session appear?
- [ ] Run mental walkthrough - can user complete full cascade?
- [ ] Check dependencies - do all "reads" actually exist?

### Example Validation
- [ ] Create example output for compliance-saas
- [ ] Verify: Does example follow template?
- [ ] Verify: Does example validate command works?

### Philosophy Alignment
- [ ] Does this serve user journey? (Not just "cool tech")
- [ ] Does this derive from previous sessions? (Generative cascade)
- [ ] Does this include critical thinking? ("What We DIDN'T Choose")
- [ ] Is this AI-optimized? (Detailed enough for autonomous execution)

---

## Common Pitfalls to Avoid

### 1. Technology-First Thinking
‚ùå **Wrong:** "Use Kubernetes for deployment"
‚úÖ **Right:** "IF journey requires >1M requests/day AND team has DevOps expertise, THEN consider Kubernetes. ELSE use Vercel/Render."

### 2. Assuming Context
‚ùå **Wrong:** "Set up authentication" (how? which method?)
‚úÖ **Right:** "Read 05-architecture.md auth strategy. IF 'JWT', generate middleware. IF 'OAuth', configure provider."

### 3. Missing Traceability
‚ùå **Wrong:** "Use blue as primary color"
‚úÖ **Right:** "From 06-brand-foundation.md: Personality is 'trustworthy' ‚Üí Use blue (trust association)"

### 4. Incomplete Decision Trees
‚ùå **Wrong:** "Choose between SQL and NoSQL"
‚úÖ **Right:** [Full decision tree with 4-5 criteria and examples]

### 5. Generic Examples
‚ùå **Wrong:** "Example: Create a User model"
‚úÖ **Right:** "Example (from compliance-saas): ComplianceOfficer model with role-based access..."

### 6. Weak "What We DIDN'T Choose"
‚ùå **Wrong:** "We didn't choose MongoDB because we chose PostgreSQL"
‚úÖ **Right:** "MongoDB would work for rapid schema iteration, but compliance documents have stable structure (regulations change slowly). Reconsider if document types become highly variable."

---

## Example: Full Command Implementation

Here's a abbreviated example showing complete implementation:

**.claude/commands/scaffold-project.md:**

```markdown
# Session 7: Scaffold Project

**Purpose:** Generate working development environment and code structure
**When to run:** After generating backlog (Session 6)
**Time required:** 30-45 minutes

## What This Session Creates

This session generates a **runnable development environment** including:
- Repository structure
- Dependency configuration
- Local development setup (Docker Compose)
- CI/CD pipeline (GitHub Actions)
- Environment configuration
- Basic code scaffolding

**Outputs:**
- `.stack-driven/07-project-scaffold.md` - Documentation of decisions
- `.stack-driven/07-project-scaffold/` - Generated code and config files

## Inputs (What This Reads)

- `.stack-driven/00-user-journey.md` - Project name, purpose
- `.stack-driven/02-tech-stack.md` - Languages, frameworks, tools
- `.stack-driven/05-architecture.md` - Monorepo? Microservices? Patterns
- `.stack-driven/08-backlog/` - What modules/services are needed

## Process

### Step 1: Determine Repository Structure

**Read:**
- Architecture (05-architecture.md) ‚Üí monorepo vs multi-repo decision
- Backlog (08-backlog/) ‚Üí how many distinct services/apps?

**Decision Tree:**

1. **How many deployable services?**
   - 1 service (web app) ‚Üí Single repo
   - 2-4 services (web + API + worker) ‚Üí Monorepo
   - 5+ services ‚Üí Multi-repo (or monorepo with orchestration)

2. **Is there shared code between services?**
   - YES ‚Üí Monorepo (or multi-repo with packages)
   - NO ‚Üí Multi-repo

3. **Team size?**
   - 1-3 people ‚Üí Monorepo (easier to manage)
   - 4-10 people ‚Üí Monorepo with module boundaries
   - 10+ people ‚Üí Consider multi-repo

**Example (compliance-saas):**
- Services: Web app, Background job processor (2 services)
- Shared code: Data models, validation logic
- Team size: 2 developers
- **Decision:** Monorepo using Turborepo

---

### Step 2: Generate Directory Structure

[Continue with remaining steps...]

## What We DIDN'T Choose (And Why)

### Microservices Architecture

**What it is:** Each feature as separate service with own database

**Why not (for this journey):**
- Journey is MVP-stage (< 1000 users expected)
- Team is 1-3 people (operational overhead too high)
- Features share data model (compliance documents accessed by multiple features)
- "Boring is beautiful" principle - monolith is simpler

**When to reconsider:**
- IF different features need independent scaling (e.g., document parsing is 100x more compute than UI)
- IF team grows >10 engineers (microservices enable team autonomy)
- IF hitting >50K requests/day (scaling bottleneck)

**Example:** Document parsing service becomes bottleneck at 10K docs/day - extract to separate service while keeping rest monolithic.

---

[Additional alternatives...]

## Quality Checklist

- [ ] Repository structure matches architecture decisions
- [ ] All tech stack tools properly configured
- [ ] Docker Compose includes all services from architecture
- [ ] CI/CD pipeline runs tests and linting
- [ ] README documents setup process (one command to run)
- [ ] .env.template includes all required variables with examples

## Next Steps

After completing this session:
1. Copy generated files to your project directory
2. Run `docker-compose up` to verify environment works
3. Proceed to `/design-database-schema` (Session 8)
4. Run `/cascade-status` to verify progress
```

---

## Ready to Implement?

### Before You Start
1. ‚úÖ Read `PHILOSOPHY.md`
2. ‚úÖ Read `REVIEW-BACKLOG.md`
3. ‚úÖ Read 2-3 existing commands to match style
4. ‚úÖ Identify which issue from backlog you're implementing

### Implementation Checklist
- [ ] Create command file (`.claude/commands/[name].md`)
- [ ] Create template file (`templates/[name].md`)
- [ ] Update `/cascade-status` command
- [ ] Update `README.md`
- [ ] Create example output (`examples/compliance-saas/`)
- [ ] Self-review against quality standards
- [ ] Test cascade integration

### Remember
**You're not just adding a command - you're completing the last 30% of an exceptional framework.**

Every command should be:
- ‚úÖ Journey-first (serves user experience)
- ‚úÖ Generative (derives from previous sessions)
- ‚úÖ Critical (includes "What We DIDN'T Choose")
- ‚úÖ AI-optimized (detailed enough for autonomous execution)
- ‚úÖ Actionable (outputs are usable, not just documentation)

---

## Questions?

If unclear about any aspect:
1. Reference existing commands for patterns
2. Check `PHILOSOPHY.md` for framework principles
3. Review `REVIEW-BACKLOG.md` for original intent
4. Default to: "Does this serve the user journey?"

**Framework mantra:**
*"User experience is the core of every product. Every decision must trace back to journey. Technology serves users, not the other way around."*

Now go build the missing 30%. üöÄ
