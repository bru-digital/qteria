# Testing Strategy

**Purpose:** Comprehensive testing strategy including unit, integration, E2E, performance, and security testing
**Generated by:** /create-test-strategy
**File location:** `product-guidelines/09-test-strategy.md`

---

## Overview

**Product**: [Product name from journey]
**Risk Level**: [High/Medium/Low - based on journey]
**Testing Approach**: [TDD/BDD/Pragmatic]
**Overall Coverage Goal**: [%]

**Journey Connection**:
This testing strategy ensures reliability for:

- Step 1: [Journey step] → [Test coverage]
- Step 2: [Journey step] → [Test coverage]
- Step 3: [Journey step] → [Test coverage]
- Step 4: [Journey step] → [Test coverage]

---

## Testing Philosophy

**Why we test**:
[Philosophy based on product risk and user value]

**What we test**:

1. [Critical path from journey] - MUST work reliably
2. [Business logic] - Data accuracy is critical
3. [Security boundaries] - Auth, authorization, data isolation
4. [Performance bottlenecks] - User experience depends on speed

**What we DON'T test**:

- Framework code (React, FastAPI internals)
- External libraries (well-tested by maintainers)
- Simple getters/setters (low value)
- Every UI state (too brittle, use E2E for user flows)

**Testing mindset**: [TDD/BDD/Pragmatic description]

---

## Unit Testing Strategy

### Scope

**What gets unit tested:**

- ✓ Business logic and algorithms
- ✓ Validation rules and business rules
- ✓ Utilities and helpers
- ✓ Complex component logic
- ✗ Framework code
- ✗ Simple presentational components
- ✗ Configuration files

### Coverage Targets

| Component       | Coverage Target | Reasoning                   |
| --------------- | --------------- | --------------------------- |
| Business logic  | [90-100%]       | Critical accuracy           |
| API handlers    | [70-80%]        | High user interaction       |
| Database models | [50-70%]        | Test custom methods         |
| UI components   | [40-60%]        | Focus on complex components |
| Utilities       | [80%]           | Reused across app           |

### Testing Tools

**Backend**:

- Test framework: [Pytest/Jest/etc. from tech stack]
- Mocking: [unittest.mock/Jest mocks]
- Coverage: [pytest-cov/c8/nyc]
- Fixtures: [pytest fixtures/factory-boy]

**Frontend**:

- Test framework: [Vitest/Jest from tech stack]
- Component testing: [Testing Library/Enzyme]
- Mocking: [MSW/nock]
- Coverage: [@vitest/coverage-v8]

### Unit Test Patterns

**Naming convention**: `test_[function]_[scenario]_[expected_result]`

**Structure**: Arrange-Act-Assert (AAA)

```
def test_assessment_score_calculates_correctly_for_perfect_match():
    # Arrange
    requirements = ["encryption", "access_control"]
    findings = ["encryption", "access_control"]

    # Act
    score = calculate_compliance_score(requirements, findings)

    # Assert
    assert score == 100
```

**Test organization**:

- Co-locate tests with source: `assessment.py` → `assessment.test.py`
- Mirror directory structure: `src/services/` → `tests/services/`

### Example Unit Tests

**Backend example:**

```[language]
[Example unit test from journey critical path]
```

**Frontend example:**

```[language]
[Example component test from journey critical path]
```

---

## Integration Testing Strategy

### Scope

**What gets integration tested:**

- ✓ API endpoints (request/response, auth, errors)
- ✓ Database operations (queries, transactions, relationships)
- ✓ External service integrations (APIs, file storage)
- ✓ Inter-service communication (if microservices)

### Integration Test Coverage

**API Tests:**

- [Endpoint 1 from journey]: Test [scenarios]
- [Endpoint 2 from journey]: Test [scenarios]
- Authentication: Test JWT validation, session handling
- Authorization: Test multi-tenant data isolation
- Rate limiting: Test per-user/team limits

**Database Tests:**

- Multi-tenant isolation: User A can't see User B's data
- Cascading operations: [Delete scenarios]
- Transaction handling: [Rollback scenarios]
- Query performance: [N+1 prevention]

**External Service Tests:**

- [Service 1]: Mock responses, test retry logic
- [Service 2]: Test [integration scenarios]

### Integration Test Setup

**Test database**:

- Tool: [Docker PostgreSQL/MongoDB/etc.]
- Migrations: Run before tests, clean after
- Isolation: [Fresh DB per test / Transactions + rollback]

**Test data**:

- Strategy: [Factories/Fixtures]
- Minimal for speed, realistic for coverage

**External APIs**:

- Strategy: [Mock with responses/MSW / Use test mode]

### Example Integration Tests

**API test:**

```[language]
[Example API test from journey - e.g., document upload]
```

**Database test:**

```[language]
[Example database test - e.g., multi-tenant isolation]
```

---

## E2E Testing Strategy

### Scope

**Critical user journeys** (from journey steps 1-3):

1. **[Journey 1 name]**:

   - [Step 1]
   - [Step 2]
   - [Step 3]
   - Expected: [Outcome]

2. **[Journey 2 name]**:

   - [Step 1]
   - [Step 2]
   - Expected: [Outcome]

3. **[Journey 3 name]**:
   - [Step 1]
   - [Step 2]
   - Expected: [Outcome]

### E2E Test Patterns

**Page Object Model**: Encapsulate page interactions
**Test data**: Use seeded data or factories
**Idempotent tests**: Each test cleans up after itself
**Stable selectors**: Use data-testid attributes
**Wait strategies**: Wait for API responses, not timeouts

### E2E Testing Tools

**Framework**: [Playwright/Cypress from tech stack]
**Visual regression**: [Percy/Chromatic - optional]
**Test data**: [Factories + database seeding]
**CI integration**: [GitHub Actions/CircleCI]

### E2E Test Execution

**Environment**: [Staging/Production-like]
**Frequency**:

- Every PR: Smoke tests ([N] critical tests, [X] min)
- Merge to main: Full suite ([N] tests, [X] min)
- Nightly: Full suite + extended scenarios

**Parallelization**: [3-5] workers
**Retries**: Retry flaky tests once
**Timeouts**: [30s] per test, [5min] per suite

### Example E2E Test

```[language]
[Example E2E test for critical journey - complete flow]
```

---

## Test Coverage and Quality Gates

### Coverage Targets

**Overall coverage**: [70%] (minimum)

**By component**:

- [Critical module 1]: [95%]
- [Critical module 2]: [90%]
- [Module 3]: [80%]
- [Module 4]: [70%]
- [UI components]: [50%]

**By test type**:

- Unit tests: [60%] of codebase
- Integration tests: [20%] (overlaps with unit)
- E2E tests: [10%] (critical paths only)

### Quality Gates

**Blocking (must pass to merge)**:

- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] Coverage >= [70%]
- [ ] No coverage decrease vs main branch
- [ ] No high/critical security vulnerabilities

**Non-blocking (warnings)**:

- [ ] E2E tests pass (block on main, warn on PR)
- [ ] Performance tests pass (informational)
- [ ] Linting passes (can be fixed later)

### CI/CD Integration

**On pull request**:

1. Run unit tests ([< 5 min])
2. Run integration tests ([< 5 min])
3. Run E2E smoke tests ([< 10 min])
4. Report coverage
5. Block merge if quality gates fail

**On merge to main**:

1. Run full test suite ([< 30 min])
2. Run full E2E suite ([< 30 min])
3. Upload coverage report

**Nightly**:

1. Run full test suite
2. Run performance tests ([1-2 hours])
3. Run security scans
4. Send report to team

**Before production deploy**:

1. All tests must pass (required)
2. Coverage check (required)
3. Security scan (required)

### Coverage Reporting

**Tool**: [Codecov/Coveralls]
**Format**: HTML (local), JSON (CI)
**Display**: Coverage badge in README
**Trends**: Track over time, alert on decreases

---

## Test Data Management

### Test Database Setup

**Local testing**: [Docker PostgreSQL/etc.]
**CI testing**: [Database service in GitHub Actions]
**Isolation strategy**: [Fresh DB per test / Transactions + rollback]
**Migrations**: Run before each test suite
**Cleanup**: [Truncate tables / Drop database]

### Test Data Creation

**Strategy**: [Factories/Fixtures]

**Factories** (recommended):

- Tool: [FactoryBoy/Fishery/Faker]
- Generate realistic data programmatically
- Easy to customize per test

**Example factory:**

```[language]
[Example factory for key entity from journey]
```

**Example factory usage:**

```[language]
[Example test using factory]
```

### Test Fixtures

**Location**: `tests/fixtures/`
**Contents**:

- Sample files: [sample.pdf, policy.docx]
- API responses: [api-responses/*.json]
- Database seeds: [seeds.sql for E2E tests]

### Test Data Cleanup

**After each test**: [Truncate tables / Rollback transaction]
**After test suite**: [Drop test database]
**CI cleanup**: [Delete test database after job]

---

## Performance Testing

### Load Testing

**Tool**: [Locust/k6/Artillery]

**Scenarios**:

1. [Scenario 1]: [X] concurrent users, [Y] requests/min
2. [Scenario 2]: [X] concurrent users, [Y] requests/min
3. [Scenario 3]: [X] concurrent users, [Y] requests/min

**Success criteria**:

- p95 latency < [500ms]
- p99 latency < [2s]
- 0% error rate under normal load
- < 5% error rate under 2x load

### Stress Testing

**Goal**: Find breaking point
**Method**: Ramp up users until system breaks
**Document**: Failure mode (graceful degradation?)

**Success criteria**:

- System handles 2x expected load
- System fails gracefully (503, not 500)

### Benchmark Testing

**Critical operations**:

- [Operation 1]: < [X]s for [scenario]
- [Operation 2]: < [X]s for [scenario]
- [Operation 3]: < [X]s for [scenario]

**Execution**:

- Before/after performance changes
- Track performance over time
- CI integration: Optional (long-running)

### Performance Test Execution

**Frequency**: Weekly (staging environment)
**Before deploy**: Run load tests
**After optimization**: Run benchmarks

---

## Security Testing

### Authentication Testing

**Scenarios**:

- Test invalid tokens (expired, malformed, wrong signature)
- Test token expiration and refresh
- Test password strength requirements
- Test rate limiting on auth endpoints

### Authorization Testing

**Scenarios**:

- Test multi-tenant isolation (User A can't access User B's data)
- Test role-based access control
- Test API endpoints require authentication
- Test public endpoints don't leak private data

### Input Validation Testing

**Scenarios**:

- Test SQL injection prevention (parameterized queries)
- Test XSS prevention (input sanitization)
- Test file upload validation (type, size limits)
- Test API input validation (400 for invalid data)

### Vulnerability Scanning

**Dependency scanning**:

- Tool: [Snyk/Dependabot/npm audit]
- Frequency: Daily (CI)
- Action: Create issue for vulnerabilities
- Blocking: High/critical block deploy

**SAST (Static Analysis)**:

- Tool: [Semgrep/Bandit/ESLint security]
- Frequency: Every PR
- Checks: Hardcoded secrets, SQL injection patterns, XSS

**DAST (Dynamic Analysis)**:

- Tool: [OWASP ZAP/Burp Suite]
- Frequency: Weekly (staging)
- Checks: Auth bypasses, injection attacks, misconfigurations

### Security Test Execution

**On PR**: Dependency scan + SAST (< 2 min)
**Weekly**: DAST scan (30-60 min)
**Before deploy**: All security tests pass

---

## Testing Workflows

### Test-Driven Development (TDD)

**Process**:

1. Write failing test (Red)
2. Write minimal code to pass (Green)
3. Refactor (still Green)
4. Repeat

**When to use**: Building new features, complex logic

**Example workflow**:

```
1. Write test: test_assessment_calculates_score()
   → FAIL (function doesn't exist)

2. Implement calculate_score()
   → PASS

3. Refactor: Extract helpers, add comments
   → PASS

4. Next test: test_assessment_handles_edge_case()
```

### Behavior-Driven Development (BDD)

**Process**:

1. Write test as user scenario (Given-When-Then)
2. Implement feature
3. Test passes

**When to use**: User-facing features, stakeholder communication

**Example**:

```
Scenario: User uploads document for assessment
  Given I am logged in
  And I am on the documents page
  When I upload a PDF file
  Then I see upload success message
  And document appears in my documents list
```

### Regression Testing

**Process**:

1. Bug discovered
2. Write test reproducing bug (FAIL)
3. Fix bug (PASS)
4. Test remains in suite (prevent regression)

**Management**:

- Never delete regression tests
- Tag with issue numbers (test_issue_123)
- Run full suite before releases

---

## Testing Checklist

### Pre-Commit Checklist

- [ ] Run unit tests locally
- [ ] Run linter
- [ ] Check test coverage (no decrease)
- [ ] Add tests for new code
- [ ] Update existing tests if behavior changed

### Pre-Merge Checklist

- [ ] All unit tests pass (CI)
- [ ] All integration tests pass (CI)
- [ ] Coverage >= [70%]
- [ ] E2E smoke tests pass
- [ ] No security vulnerabilities
- [ ] Code review approved

### Pre-Deploy Checklist

- [ ] All tests pass (unit, integration, E2E)
- [ ] Performance tests pass
- [ ] Security scan clean
- [ ] Database migrations tested
- [ ] Rollback plan ready
- [ ] Monitoring alerts configured

### Production Monitoring

- [ ] Error rate alerts configured
- [ ] Performance degradation alerts
- [ ] Test production monitoring (synthetic tests)
- [ ] Incident response runbook ready

---

## What We DIDN'T Choose (And Why)

### [Alternative 1]

**What it is**: [Description]

**Why not (for this journey)**:

- [Reason 1 based on journey]
- [Reason 2 based on tech stack]
- [Reason 3 based on pragmatism]

**When to reconsider**:

- IF [condition 1]
- IF [condition 2]
- IF [condition 3]

**Example**: [Concrete example from journey context]

---

### [Alternative 2]

**What it is**: [Description]

**Why not (for this journey)**:

- [Reason 1 based on journey]
- [Reason 2 based on team]
- [Reason 3 based on cost]

**When to reconsider**:

- IF [condition 1]
- IF [condition 2]

**Example**: [Concrete example]

---

### [Alternative 3]

**What it is**: [Description]

**Why not (for this journey)**:

- [Reason 1]
- [Reason 2]
- [Reason 3]

**When to reconsider**:

- IF [condition 1]
- IF [condition 2]

**Example**: [Concrete example]

---

### [Alternative 4]

**What it is**: [Description]

**Why not (for this journey)**:

- [Reason 1]
- [Reason 2]

**When to reconsider**:

- IF [condition 1]
- IF [condition 2]

**Example**: [Concrete example]

---

## Setup Instructions

### Backend Setup ([Python/Node.js/etc.])

```bash
# Install testing dependencies
[commands]

# Configure test runner
[configuration steps]

# Run tests
[test commands]

# View coverage
[coverage commands]
```

### Frontend Setup ([React/Vue/etc.])

```bash
# Install testing dependencies
[commands]

# Configure test runner
[configuration steps]

# Run tests
[test commands]

# View coverage
[coverage commands]
```

### CI/CD Setup

```yaml
# [.github/workflows/test.yml or equivalent]
[CI configuration for running tests]
```

---

## Testing Summary

### Coverage Summary

- **Overall coverage**: [70%]
- **Unit tests**: [X] tests covering [Y%]
- **Integration tests**: [X] tests covering [Y%]
- **E2E tests**: [X] tests covering [Y] critical journeys
- **Performance tests**: [X] scenarios
- **Security tests**: [X] checks

### Quality Gates Summary

**Blocking**:

- All unit/integration tests pass
- Coverage >= [70%]
- No high/critical vulnerabilities

**Non-blocking**:

- E2E tests (block on main)
- Performance tests (informational)

### Testing Principles

1. **Test what matters**: Critical paths, business logic, security
2. **Fast feedback**: Unit/integration tests < 10 min
3. **Pragmatic coverage**: 70% focused on critical code, not 100% of everything
4. **Quality gates**: Prevent bad code from reaching production
5. **Continuous improvement**: Track coverage trends, add tests for bugs

---

## Validation

This testing strategy is complete when:

- [ ] All journey critical paths have E2E tests
- [ ] Business logic has unit tests (90%+ coverage)
- [ ] API endpoints have integration tests
- [ ] Test coverage targets are defined and enforced
- [ ] CI/CD integration is configured
- [ ] Test data management approach is defined
- [ ] Performance testing plan is defined
- [ ] Security testing plan is defined
- [ ] "What We DIDN'T Choose" section explains 4+ alternatives

---

## References

- **User Journey**: `product-guidelines/00-user-journey.md`
- **Tech Stack**: `product-guidelines/02-tech-stack.md`
- **Architecture**: `product-guidelines/05-architecture.md`
- **Backlog**: `product-guidelines/07-backlog/BACKLOG.md`
- **Scaffold**: `product-guidelines/07-project-scaffold.md`
- **Test Configuration**: `product-guidelines/09-test-strategy/` (generated files)

---

**Generated**: [Date]
**Testing Approach**: [TDD/BDD/Pragmatic]
**Overall Coverage Goal**: [70%]
**Ready for**: Implementation
